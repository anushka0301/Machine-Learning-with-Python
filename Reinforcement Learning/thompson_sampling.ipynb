{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.3 64-bit ('base': conda)",
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "3a5f798d4741801a68b89d4058d52844897cf1b72d63ff64034d8e498e867c5f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "**Thompson Sampling** is a simple **reinforcement** algorithm. In reinforcement learning, the agent generates its training data by interacting with the world. The agent learns the consequences of its actions through trial and error, instead of being fed explicity.\n",
    "\n",
    "A very popular use of the UCB algorithm is determining the advertisement that produces the maximum reward, netflix item based recommender systems, bidding and stock exchange, traffic light control and automation in industries.\n",
    "\n",
    "The basic algorithm is: <br/>\n",
    "- Step 1: At each round n, we consider two numbers for each  i,<br/>\n",
    "        i. the number of times i was got reward 1 upto round n<br/>\n",
    "        ii. the number of times i was got reward 0 upto round n\n",
    "- Step 2: From each ad i, we take a random draw from distribution<br/>\n",
    "- Step 3: We select the i with highest distribution"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "First we import the libraries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "source": [
    "Then, we import the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.read_csv('ad.csv') #Use whatever dataset is available to you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Then,we implement the Thompson Sampling algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "N=10000 #Total number of times we advertise\n",
    "d=10 #Total number of ads\n",
    "ads_selected=[]\n",
    "number_of_rewards_1=[0]*d\n",
    "number_of_rewards_0=[0]*d\n",
    "total_reward=0\n",
    "for n in range(0,N):\n",
    "    ad=0\n",
    "    max_random=0\n",
    "    for i in range(0,d):\n",
    "        random_beta=random.betavariate(number_of_rewards_1[i]+1,number_of_rewards_0[i]+1)\n",
    "        if(random_beta > max_random):\n",
    "            max_random=random_beta\n",
    "            ad=i\n",
    "    ads_selected.append(ad)\n",
    "    reward=dataset.values[n,ad]\n",
    "    if reward==1:\n",
    "        number_of_rewards_1[ad]+=1\n",
    "    else:\n",
    "        number_of_rewards_0[ad]+=1\n",
    "    total_reward=total_reward+reward"
   ]
  },
  {
   "source": [
    "At last, we visualize the results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ads_selected)\n",
    "plt.title('Histogram of Ads seletions')\n",
    "plt.xlabel('Ads selected')\n",
    "plt.ylabel('Number of times each ad was seected')\n",
    "plt.show()"
   ]
  }
 ]
}