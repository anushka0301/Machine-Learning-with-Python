{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.3 64-bit ('base': conda)",
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "3a5f798d4741801a68b89d4058d52844897cf1b72d63ff64034d8e498e867c5f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "**Upper Confidence Bound** is a simple reinforcement algorithm. In **reinforcement learning**, the agent generates its training data by interacting with the world. The agent learns the consequences of its actions through trial and error, instead of being fed explicity.\n",
    "\n",
    "\n",
    "A very popular use of the UCB algorithm is determining the advertisement that produces the maximum reward.\n",
    "\n",
    "\n",
    "Imagine an online advertising trial where an advertiser wants to measure the click-through rate of three different ads for the same product. Whenever a user visits the website, the advertiser displays an ad at random. The advertiser then monitors whether the user clicks on the ad or not. After a while, the advertiser notices that one ad seems to be working better than the others. The advertiser must now decide between sticking with the best-performing ad or continuing with the randomized study.\n",
    "If the advertiser only displays one ad, then he can no longer collect data on the other two ads. Perhaps one of the other ads is better, it only appears worse due to chance. If the other two ads are worse, then continuing the study can affect the click-through rate adversely. This advertising trial exemplifies decision-making under uncertainty.\n",
    "In the above example, the role of the agent is played by an advertiser. The advertiser has to choose between three different actions, to display the first, second, or third ad. Each ad is an action. Choosing that ad yields some unknown reward. Finally, the profit of the advertiser after the ad is the reward that the advertiser receives.\n",
    "\n",
    "\n",
    "\n",
    "The basic algorithm is: <br/>\n",
    "- Step 1: At each round n, we consider two numbers for each ad i,<br/>\n",
    "        i. the number of times i was selected upto round n<br/>\n",
    "        ii. the sum of rewards of the ad i upto round n\n",
    "- Step 2: From these two numbers we compute:<br/>\n",
    "        i. the average reward of i upto round n<br/>\n",
    "        ii. the confidence level at round n\n",
    "- Step 3: We select the i with maximum upper confidence bound\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "First we import the libraries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "source": [
    "Then, we import the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.read_csv('ad.csv') #Use whatever dataset is available to you"
   ]
  },
  {
   "source": [
    "Then, we implement the UCB algorithm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=10000 #Total number of times we advertise\n",
    "d=10 #Total number of ads\n",
    "ads_selected=[]\n",
    "number_of_selections=[0]*d\n",
    "sums_of_rewards=[0]*d\n",
    "total_reward=0\n",
    "for n in range(0,N):\n",
    "    ad=0\n",
    "    max_upperbound=0\n",
    "    for i in range(0,d):\n",
    "        if(number_of_selections[i]>0):\n",
    "            average_reward=sums_of_rewards[i]/number_of_selections[i]\n",
    "            delta_i=math.sqrt(3/2*math.log(n+1)/number_of_selections[i])\n",
    "            upper_bound=average_reward+delta_i\n",
    "        else:\n",
    "            upper_bound=1e400 #We do this to select each ad atleast once the first time\n",
    "        if(upper_bound > max_upperbound):\n",
    "            max_upperbound=upper_bound #Select the ad with the maximum upper confidence bound\n",
    "            ad=i\n",
    "    ads_selected.append(ad) \n",
    "    number_of_selections[ad]+=1 #Increase the number of selections for the selected ad\n",
    "    reward=dataset.values[n,ad] #Receive rewards as per simulated dataset\n",
    "    sums_of_rewards[ad]+=reward #Calculate the total rewards for the selected ad\n",
    "    total_reward=total_reward+reward"
   ]
  },
  {
   "source": [
    "At last, we visualize the results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ads_selected)\n",
    "plt.title('Histogram of Ads seletions')\n",
    "plt.xlabel('Ads selected')\n",
    "plt.ylabel('Number of times each ad was seected')\n",
    "plt.show()"
   ]
  }
 ]
}